<h1>Goal</h1>
<p>Write an HBase WordCount program to count  all unique terms&rsquo; occurrences from the clueWeb09 dataset. Each row record of  columnfamily &ldquo;frequencies&rdquo; is unique; the rowkey is the unique term  stored in byte format, column name is &ldquo;count&rdquo; and value is the term  frequency shown in all documents. Load the result to HBase WordCountTable.  Figure 1 shows the schema of WordCountTable.<br /></p>
  <p align="center"><img src="assets/img/hw/se2/1.png" alt="" width="259" height="177" /><br />
  <em>Figure 1.</em> WordCount table schema for storing unique  term&rsquo;s occurrences</p>
<h1>Deliverables </h1>
<p>Zip your source code and report in  a file named <em>username</em>_project4.zip</p>
<h1>Evaluation</h1>
<div>
    <p>The point total for this project is 5, where the
        distribution is as follows:</p>
    <ol type="a">
        <li>Completeness of your code and output (3 points)</li>
        <li>Correctness of written report (2 points)</li>
    </ol>
</div>

<h1>Introduction </h1>
<p>WordCount is a simple program which counts  the number of occurrences of each word in a given
    text input dataset. It fits  very well with the map/reduce programming model, making WordCount a
    great example  to understand the Hadoop MapReduce programming style [1]. Instead of  loading the
    data from HDFS, we will load our data directly from existing HBase  records which store the similar
    content structures on HBase and HDFS (if you finished Homework 6, this has already been done).
</p>
<p>In this homework and the next homework (Building an Inverted Index) we use the same source code, which can be found in:</p>
<div>
<pre>/root/MoocHomeworks/HBaseWordCount</pre>
</div>
<h2>HBase</h2>
<p>For details about  HBase [2], please see Homework 5: Load Data into HBase</p>
<h2>Clueweb09 dataset</h2>
<p>We are using the ClueWeb09 dataset, which was  created to support research on information retrieval and related human language  technologies. It consists of about 1 billion webpages in ten languages that  were collected in January and February 2009. The dataset is used by several  tracks of the <a href="http://trec.nist.gov/">TREC</a> conference [3].  Since the ClueWeb09 dataset is composed of webpages crawled from the Internet, the  uploaded table schemas are designed as shown in Figure 2.<br />
  </p>
  <p align="center"><img src="assets/img/hw/se2/2.png" alt="" width="624" height="92" /><br />
  <em>Figure 2</em>. Data table schema for storing the  ClueWeb09 dataset<br /></p>
  <p>So, while similar to Hadoop WordCount [4], the  differences are that data is stored on HBase and URI is the &ldquo;filename&rdquo; that contains  all the text content.</p>
<h2>Mapper, Reducer and Main Program </h2>
<p>Now we are going to implement the HBase  WordCount. Our implementation consists of three main parts:</p>
<ul>
  <li>Mapper</li>
  <li>Reducer</li>
  <li>Main  program</li>
</ul>
<h3>Mapper</h3>
<p>A Mapper overrides the &ldquo;map&rdquo; function from  the Class &quot;org.apache.hadoop.hbase.mapreduce.TableMapper&lt;Text,  LongWritable&gt;&quot; which provides &lt;key, value&gt; pairs as the input. A  Mapper implementation may output &lt;key, value&gt; pairs using the provided  Context.<br />
  &lt;key, value&gt; of this map function is  &lt;rowkey, content&gt;, where the key  is the rowkey of an HBase record related to a specified URI, and the content is  the stored text of that URI. Your Map task should output &lt;word, frequency&gt;  for each word in the content of text.<br />
  <em>Pseudocode</em></p>

<div>
    <pre>
void Map (key, value){
    for each word x in the content of a hbase record:
    context.write(x, freq);
}
    </pre>
</div>
<p><em>Detail implementation</em></p>

<div><br />
    <script src="https://gist.github.com/cloudmooc/6234757.js"></script>
</div>

<h3>Reducer </h3>
<p>A Reducer collects the intermediate &lt;key,  value&gt; output from multiple map tasks and assembles a single result. Here,  the reducer function will sum up the occurrence of each word to pairs as  &lt;word, occurrence&gt;, then write it back to an HBase table with put operations  which contain the key-value pair information of each word.<br />
  <em>Pseudo-code</em></p>
<div>
    <pre>
void Reduce (keyword, &lt;list of value&gt;){
    for each x in &lt;list of value&gt;:
        sum+=x;
        context.write(rowkey(x), freq);
}
    </pre>
</div>
<p><em>Detail-implementation</em></p>


<div>
    <script src="https://gist.github.com/cloudmooc/6234769.js"></script>
</div>

<h3>Main program </h3>
<p>The main function has been provided as standard initialization, although you can modify it to suit your own style. Hint:  the provided code is designed for using put operations in the reducer  content.write() function. Before writing the codes, please read the HBase MapReduce tutorial first. [5].</p>
<h1>Edit</h1>
<p>The sketch code is stored within the  provided VirtualBox image <a href="http://cloudmooc.soic.indiana.edu:8080/homework?unit=0">Environment Setup</a> [6]. You may use linux text editor vi/vim to add  your code.</p>
<div class="dp-highlighter">
        <div class="bar">
<pre>
$ cd /root/MoocHomeworks/HBaseWordCount/
$ vim src/iu/pti/hbaseapp/clueweb09/WordCountClueWeb09.java
</pre>
        </div>
  </div>
<h2>Compile and run your code</h2>
<p>For your convenience, we have provided a one-click script compileAndExecWordCount.sh for compiling and execution. Standard error messages such as &ldquo;compile errors,  execution errors, etc.&rdquo; will be redirected on the screen. You may debug it  based on the returned messages.</p>
<div class="dp-highlighter">
        <div class="bar">
<pre>
$ cd /root/MoocHomeworks/HBaseWordCount
$ ./compileAndExecWordCount.sh
</pre>
        </div>
</div>
<h2>View the result</h2>
<p>The result is  generated as /root/MoocHomeworks/HBaseWordCount/output/project1.txt. </p>

<div class="dp-highlighter">
        <div class="bar">
<pre>
$ cd /root/MoocHomeworks/HBaseWordCount
$ cat output/project1.txt
</pre>
        </div>
  </div>
<h1>What&rsquo;s next?</h1>
<p>You will be ready  to program your next project: &ldquo;Parallel Inverted Index Building on HBase.&rdquo;</p>
<h1>References</h1>
<ol>
  <li>MapReduce Tutorial: <a href="http://hadoop.apache.org/docs/stable/mapred_tutorial.html">http://hadoop.apache.org/docs/stable/mapred_tutorial.html</a></li>
  <li>HBase  official website: <a href="http://hbase.apache.org/">http://hbase.apache.org/</a></li>
  <li>Clueweb09  dataset: <a href="http://lemurproject.org/clueweb09/">http://lemurproject.org/clueweb09/</a></li>
  <li>Cloud MOOC Homework 1 - Hadoop WordCount: <a href="http://cloudmooc.soic.indiana.edu:8080/homework?unit=1">http://cloudmooc.soic.indiana.edu:8080/homework?unit=1</a></li>
  <li>HBase MapReduce Examples: <a href="http://hbase.apache.org/book/mapreduce.example.html">http://hbase.apache.org/book/mapreduce.example.html</a></li>
  <li>Cloud  MOOC VirtualBox: http://cloudmooc.soic.indiana.edu:10002/mooc-ubuntu1204.ova</li>
  <li>Java API, <a href="http://docs.oracle.com/javase/6/docs/api/">http://docs.oracle.com/javase/6/docs/api/</a></li>
  <li>Hadoop API, <a href="http://hadoop.apache.org/docs/r1.1.2/api/">http://hadoop.apache.org/docs/r1.1.2/api/</a></li>
  <li>HBase API, <a href="http://hbase.apache.org/0.94/apidocs/">http://hbase.apache.org/0.94/apidocs/</a></li>
</ol>
