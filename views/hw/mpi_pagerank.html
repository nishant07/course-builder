<div id="accordion">
	<h1>
		Deliverables
	</h1>
	<div>
		<p>
			You are required to turn in the following items in a zip file (<strong>groupID_Project1P2.zip</strong>)
			in this assignment:
		</p>
		<ol>
			<li>The source code of parallel PageRank you implemented.</li>
			<li>The executable class file and the README file that describes its
				usage.</li>
			<li>Technical report (<strong>groupID_Project1P2_report.docx</strong>)
				that contains:
				<ol type="a">
					<li>The description of the main steps and data flow in your
						program.</li>
					<li>The output file (<strong>groupID_pagerank_output.txt</strong>)
						which contains the top 10 ranking url numbers.
					</li>
				</ol>
			</li>
		</ol>
		<p>
			Note that this document only provides MPI C/C++ instructions. If you
			prefer or know MPJExpress <a href="http://mpj-express.org/">http://mpj-express.org/</a>,
			please feel free to implement it.
		</p>
		<p>Points will be reduced (maximum 0.5 points) if the filename or
			directory structure are different from instructed above.</p>
	</div>

	<h1>Evaluation</h1>
	<div>
		<p>The point total for Project #1 Part 1 is 5, where the
			distribution is as follows:</p>
		<ol type="a">
			<li>Completeness of your code and output (3 points)</li>
			<li>Correctness of written report (1.5 points)</li>
			<li>Readability and clarity of README.txt (0.5 points)</li>
		</ol>
	</div>

	<h1>Introduction</h1>
	<div>
		<p>In the second part of Project #1, you will implement a parallel
			version of PageRank by using MPI programming interface. We provide a
			pseudocode for MPI PageRank and you need to complete the
			implementation based on the pseudocode to make a working version of
			it. We provide functions for file IO so that you can focus on main
			implementation only. For more details on the PageRank algorithm, you can
			refer to the previous descriptions for part 1 of Project #1.</p>

		<p>
			<strong>Parallel PageRank</strong>
		</p>
		<p>Developing parallel PageRank is an active research area for
			both industry and academia, and numerous algorithms have been
			proposed. The key idea is to
			partition PageRank problems into N sub-problems so that N processors
			solve each sub-problem concurrently. One of the simplest approaches in
			partitioning is vertex-centric. The graph can
			be divided into groups of vertices and each group will be handled
			by a processor. We take this approach for our MPI PageRank
			implementation.</p>
	</div>

	<h1>Pseudocode for MPI PageRank</h1>




	<div>
		<p>You need to complete two files: mpi_main.c and mpi_pagerank.c.
			Pseudocodes are as follows:</p>
		<p>
			<br> <strong>mpi_main.c </strong>
		</p>

		<!-- HTML generated using hilite.me -->
		<div
			style="background: #ffffff; overflow: auto; width: auto;">
			<pre style="margin: 0; line-height: 125%">
				<span style="color: #507090">#include &lt;mpi.h&gt; </span>
<span style="color: #507090">#include &quot;pagerank.h&quot; </span>
 
<span style="color: #303090; font-weight: bold">int</span> <span
					style="color: #0060B0; font-weight: bold">main</span>(<span
					style="color: #303090; font-weight: bold">int</span> argc, <span
					style="color: #303090; font-weight: bold">char</span> <span
					style="color: #303030">**</span>argv) 
{ 
    <span style="color: #808080">/* Definition of data structure and variables for MPI PageRank */</span> 
    <span style="color: #303090; font-weight: bold">int</span> numUrls, totalNumUrls; 
    <span style="color: #303090; font-weight: bold">char</span> <span
					style="color: #303030">*</span>filename; 
    <span style="color: #303090; font-weight: bold">int</span> <span
					style="color: #303030">**</span>am_index; <span
					style="color: #808080">/* int[numUrls][2] */</span> 
                    <span style="color: #808080">/* am_index[i][0]refers to second index for am,  am_index[i][1] refers to length of target urls list */</span> 
    <span style="color: #303090; font-weight: bold">int</span> <span
					style="color: #303030">*</span>adjacency_matrix;      <span
					style="color: #808080">/* [numTotalWebPages] */</span> 
    <span style="color: #303090; font-weight: bold">double</span> <span
					style="color: #303030">*</span>rank_values_table;  <span
					style="color: #808080">/* [numUrls] */</span> 
    <span style="color: #303090; font-weight: bold">double</span> threshold; 
   
    <span style="color: #303090; font-weight: bold">int</span> rank, nproc, mpi_namelen; 
    <span style="color: #303090; font-weight: bold">char</span> mpi_name[MPI_MAX_PROCESSOR_NAME]; 
 
    <span style="color: #808080">/* MPI Initialization */</span> 
    MPI_Init(<span style="color: #303030">&amp;</span>argc, <span
					style="color: #303030">&amp;</span>argv); 
    MPI_Comm_rank(MPI_COMM_WORLD, <span style="color: #303030">&amp;</span>rank); 
    MPI_Comm_size(MPI_COMM_WORLD, <span style="color: #303030">&amp;</span>nproc); 
    MPI_Get_processor_name(mpi_name, <span style="color: #303030">&amp;</span>mpi_namelen); 
 
    <span style="color: #808080">/* Parse command line arguments */</span> 
 
    <span style="color: #808080">/* Read local adjacency matrix from file for each process */</span> 
    mpi_read(filename, <span style="color: #303030">&amp;</span>numUrls, <span
					style="color: #303030">&amp;</span>am_index, <span
					style="color: #303030">&amp;</span>adjacency_matrix, MPI_COMM_WORLD); 

    <span style="color: #808080">/* Set totalNumUrls */</span> 
    MPI_Allreduce(<span style="color: #303030">&amp;</span>numUrls, <span
					style="color: #303030">&amp;</span>totalNumUrls, <span
					style="color: #0000D0; font-weight: bold">1</span>, MPI_INT, MPI_SUM,MPI_COMM_WORLD); 
 
    <span style="color: #808080">/* Global page rank value table */</span> 
    rank_values_table <span style="color: #303030">=</span> (<span
					style="color: #303090; font-weight: bold">double</span> <span
					style="color: #303030">*</span>) malloc(totalNumUrls <span
					style="color: #303030">*</span> <span
					style="color: #008000; font-weight: bold">sizeof</span>(<span
					style="color: #303090; font-weight: bold">double</span>)); 
    assert(rank_values_table <span style="color: #303030">!=</span> <span
					style="color: #007020">NULL</span>); 
 
    <span style="color: #808080">/* Root(rank 0) computes the initial rank value for each web page */</span> 
 
    <span style="color: #808080">/* Broadcast the initial rank values to all other compute nodes */</span> 
    MPI_Bcast(rank_values_table, totalNumUrls, MPI_DOUBLE, <span
					style="color: #0000D0; font-weight: bold">0</span>, MPI_COMM_WORLD);        
 
    <span style="color: #808080">/* Start the core computation of MPI PageRank */</span> 
    mpi_pagerank(adjacency_matrix, am_index, numUrls, totalNumUrls, 
                 num_iterations, threshold, rank_values_table, 
                 MPI_COMM_WORLD); 
 
    <span style="color: #808080">/* Save results to a file */</span> 
     
    <span style="color: #808080">/* Release resources e.g. free(adjacency_matrix); */</span> 
 
    MPI_Finalize(); 
    <span style="color: #008000; font-weight: bold">return</span> (<span
					style="color: #0000D0; font-weight: bold">0</span>); 
} 
</pre>
		</div>

		<p>
			<br> <strong>mpi_pagerank.c </strong>
		</p>

		<!-- HTML generated using hilite.me -->
		<div
			style="background: #ffffff; overflow: auto; width: auto;">
			<pre style="margin: 0; line-height: 125%">
				<span style="color: #507090">#include &lt;mpi.h&gt; </span>
<span style="color: #507090">#include &quot;pagerank.h&quot; </span>
 
<span style="color: #808080">/* Define mpi_pagerank function with following signature */</span> 
<span style="color: #303090; font-weight: bold">int</span> <span
					style="color: #0060B0; font-weight: bold">mpi_pagerank</span>( 
    <span style="color: #303090; font-weight: bold">int</span> adjacency_matrix, <span
					style="color: #808080">/* adjacency matrix for pagerank */</span>  
    <span style="color: #303090; font-weight: bold">int</span> <span
					style="color: #303030">**</span>am_index, <span
					style="color: #808080">/* index array for adjacency matrix */</span> 
    <span style="color: #303090; font-weight: bold">int</span> numUrls,        <span
					style="color: #808080">/* num of urls assigned to local machine */</span> 
    <span style="color: #303090; font-weight: bold">int</span> total_num_urls, <span
					style="color: #808080">/* num of total urls */</span> 
    <span style="color: #303090; font-weight: bold">int</span> num_iterations, <span
					style="color: #808080">/* num of maximum interations */</span> 
    <span style="color: #303090; font-weight: bold">double</span> threshold,   <span
					style="color: #808080">/* control the number of iterations */</span> 
    <span style="color: #303090; font-weight: bold">double</span> <span
					style="color: #303030">*</span>rank_values_table, <span
					style="color: #808080">/* double[total_num_urls] */</span> 
    MPI_Comm comm)             <span style="color: #808080">/* MPI communicator */</span> 
{ 
    <span style="color: #808080">/* Definitions of variables */</span> 
 
    <span style="color: #808080">/* Get MPI rank */</span> 
    MPI_Comm_rank(comm, <span style="color: #303030">&amp;</span>rank); 
 
    <span style="color: #808080">/* Allocate memory and initialize values for local_rank_values_table */</span> 
 
    <span style="color: #808080">/* Start computation loop */</span> 
    <span style="color: #008000; font-weight: bold">do</span> 
    { 
        <span style="color: #808080">/* Compute pagerank and dangling values */</span> 
 
        <span style="color: #808080">/* Distribute pagerank values */</span> 
        MPI_Allreduce(local_rank_values_table, rank_values_table, 
                      total_num_urls, MPI_DOUBLE, MPI_SUM, comm); 
 
        <span style="color: #808080">/* Distribute dangling values */</span> 
        MPI_Allreduce(<span style="color: #303030">&amp;</span>dangling, <span
					style="color: #303030">&amp;</span>sum_dangling, <span
					style="color: #0000D0; font-weight: bold">1</span>, MPI_DOUBLE, MPI_SUM, comm); 
         
        <span style="color: #808080">/* Recalculate the page rank values with damping factor 0.85 */</span> 
 
        <span style="color: #808080">/* Root(process 0) computes delta to determine to stop or continue */</span> 
 
        <span style="color: #808080">/* Root broadcasts delta */</span> 
        MPI_Bcast(<span style="color: #303030">&amp;</span>delta, <span
					style="color: #0000D0; font-weight: bold">1</span>, MPI_DOUBLE, <span
					style="color: #0000D0; font-weight: bold">0</span>, MPI_COMM_WORLD); 
    } 
    <span style="color: #008000; font-weight: bold">while</span> (delta <span
					style="color: #303030">&gt;</span> threshold <span
					style="color: #303030">&amp;&amp;</span> loop<span
					style="color: #303030">++</span> <span style="color: #303030">&lt;</span> num_iterations); 
    <span style="color: #008000; font-weight: bold">return</span> <span
					style="color: #0000D0; font-weight: bold">1</span>; 
} 	
</pre>
		</div>
	</div>

	<h1>FILE IO functions</h1>
	<div>
		<p>Besides pseudocodes, we provide two MPI functions (mpi_read
			and mpi_write) for file reading and writing. This is so you can simply
			call them without re-implementation.</p>
		<p>
			<strong>mpi_read </strong>
		</p>
		<p>This function is to read an adjacency matrix file and assign a
			local matrix to each process. After this call each process will have
			its own adjacency matrix, and therefore each should call this
			function. The following is a signature of mpi_read function:</p>

		<!-- HTML generated using hilite.me -->
		<div
			style="background: #ffffff; overflow: auto; width: auto;">
			<pre style="margin: 0; line-height: 125%">
				<span style="color: #808080">/*  </span>
<span style="color: #808080"> * Read an adjacency matrix file and assign a local matrix to each process </span>
<span style="color: #808080"> * Note </span>
<span style="color: #808080"> * -. After this call, each process will have its own adjacency matrix </span>
<span style="color: #808080"> * -. Each process will have roughly eqaul amount of number of urls </span>
<span style="color: #808080"> */</span>  
<span style="color: #303090; font-weight: bold">int</span> mpi_read(<span
					style="color: #303090; font-weight: bold">char</span> <span
					style="color: #303030">*</span>filename,    <span
					style="color: #808080">/* (IN) file name */</span> 
             <span style="color: #303090; font-weight: bold">int</span> <span
					style="color: #303030">*</span>numUrls,      <span
					style="color: #808080">/* (OUT) number of urls assigned to local machine */</span> 
             <span style="color: #303090; font-weight: bold">int</span> <span
					style="color: #303030">***</span>am_index,   <span
					style="color: #808080">/* (OUT) index array for adjacency matrix, assigned to local machine */</span> 
             <span style="color: #303090; font-weight: bold">int</span> <span
					style="color: #303030">**</span>adjacency_matrix,    <span
					style="color: #808080">/* (OUT) adjacency matrix assigned to local machine */</span> 
             MPI_Comm comm);    <span style="color: #808080">/* (IN) MPI communicator */</span> 
</pre>
		</div>


		<p>
			<br>
			<strong>mpi_write </strong>
		</p>
		<p>This function is for saving rank values to a file. We assume
			rank values are synchronized through all processes. Therefore it
			doesn&#8217;t matter which process you choose to write. In this function, 
			we simply have root (rank 0) processes to write rank values to a file.</p>

		<!-- HTML generated using hilite.me -->
		<div
			style="background: #ffffff; overflow: auto; width: auto;">
			<pre style="margin: 0; line-height: 125%">
				<span style="color: #808080">/*  </span>
<span style="color: #808080"> * Save rank_values_table[] to a file </span>
<span style="color: #808080"> * Note </span>
<span style="color: #808080"> * -. Only root (rank 0) will save rank values to a file </span>
<span style="color: #808080"> * -. Assume rank_values_table[] are the same across all proc  </span>
<span style="color: #808080"> */</span> 
<span style="color: #303090; font-weight: bold">int</span> mpi_write(<span
					style="color: #303090; font-weight: bold">char</span> <span
					style="color: #303030">*</span>filename,   <span
					style="color: #808080">/* (IN) file name */</span> 
              <span style="color: #303090; font-weight: bold">int</span> totalNumUrls, <span
					style="color: #808080">/* (IN) number of total urls */</span> 
              <span style="color: #303090; font-weight: bold">double</span> <span
					style="color: #303030">*</span>rank_values_table,  <span
					style="color: #808080">/* (IN) array of rank values. double[total_num_urls] */</span> 
              MPI_Comm comm);   <span style="color: #808080">/* (IN) MPI communicator */</span> 
</pre>
		</div>

	</div>

	<h1>Compile guide</h1>
	<div>
		<p>Instructions for compiling and a recommended execution guide are
			as follows (we assume that you will compile and run on machines of the CS
			department, such as burrow cluster):</p>
		<p>
			<strong>Step one: Compile </strong>
		</p>
		<p>To compile MPI code, you need to set up correct MPI development
			environments. You can do that by using an &#8220;mpi-selector&#8221;
			command:</p>
		<p>First, check which MPI versions are available:</p>
		<pre>$ mpi-selector &#8211;list</pre>
		<p>In Silo, you can see the following list:</p>
		<pre>lam-i386 
lam-x86_64 
openmpi-1.4-gcc-i386 
openmpi-1.4-gcc-x86_64 
</pre>
		<p>Second, choose one of the MPI versions in the list by executing
			the following command:</p>
		<pre>$ mpi-selector --set openmpi-1.4-gcc-i386</pre>
		<p>You may see a warning for overwriting your previous setting.
			You can answer 'yes'.</p>
		<p>Next you will see 'Defaults already exist; overwrite them (Y/N)?' Choose 'Yes'.</p>
		<p>Now you are ready to compile the sample simply by using the
			&#8220;make&#8221; command (we provide Makefile for you).</p>
		<pre>
$ make clean
$ make 
</pre>
		<p>You will see an executable &#8220;mpi_main&#8221; in the
			directory.</p>
		<p>
			<strong>Step two: Execution </strong>
		</p>
		<p>The executable you need to make should take a few parameters as
			an input. Most importantly we should be able to specify input file by
			using &#8220;-i&#8221; option and the number of iterations by using
			&#8220;-n&#8221; option. The full list of options you need to
			implement is as follows:</p>
		<pre>
$ mpi_main -h 
Usage: mpi_main -i filename -n num_iterations 
       -i filename    : adjacency matrix input file 
       -n num_iterations: number of iterations 
       -t threshold   : threshold value (default 0.0010) 
       -o             : output timing results (default yes) 
       -d             : enable debug mode 
       -h             : print help information 
		</pre>

		<p>
			To run &#8220;mpi_main&#8221;, you need to use a MPI command called
			&#8220;mpirun&#8221;. For example, to process
			&#8220;pagerank.input&#8221; file (included as a sample input) by
			using 2 processes concurrently, you can type the following commands:
			<br /> <br />
		</p>
		<pre>$ mpirun -np 2 mpi_main -i pagerank.input -n 10 -t 0.001</pre>
		<p>
			<br /> The following is an example of MPI PageRank execution. <br />
			<br />
		</p>
		<pre>
max_iterations=10, threshold=0.001000000 
  ->cur_iteration=0 delta=0.137787065 
  ->cur_iteration=1 delta=0.101166196 
  ->cur_iteration=2 delta=0.033934278 
  ->cur_iteration=3 delta=0.026608634 
  ->cur_iteration=4 delta=0.018108631 
  ->cur_iteration=5 delta=0.014472174 
  ->cur_iteration=6 delta=0.010200567 
  ->cur_iteration=7 delta=0.007725284 
  ->cur_iteration=8 delta=0.005532020 
  ->cur_iteration=9 delta=0.004065336 
  ->cur_iteration=10 delta=0.002924901 
Proc:0 is writing rank values of 11 urls to file pagerank.output 
 
  **** MPI PageRank **** 
Num of processes   = 2 
Input file         = pagerank.input 
totalNumUrls       = 11 
num_itertaions     = 10 
threshold          = 0.001000000 
I/O time           =     0.4031 sec 
Computation timing =     0.0002 sec 	
		</pre>
		</div>

	</div>
