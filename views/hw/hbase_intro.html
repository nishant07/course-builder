<h1>Goal</h1>
<p>This is a warm-up  practice to provide background knowledge of Hadoop MapReduce and HBase Database/Datastore. The following practice will cover:</p>
<ul>
  <li>Overview  for Hadoop and HBase</li>
  <li>Set up  the environment for Hadoop and HBase</li>
  <li>Create  HBase Database tables for ClueWeb09 [1] data, and load them into HBase</li>
  <li>Test  the stored HBase records with the provided java executable</li>
</ul>
<h1>Deliverables </h1>
<p>Submit an output  file dataTable1.txt containing your results to the Google MOOC submission page by using the  provided java class iu.pti.hbaseapp.HBaseTableReader.</p>
<h1>Evaluation</h1>
<div>
    <p>None</p>
</div>
<h1>Introduction </h1>
<p>The development of data-intensive problems  in recent years has brought new requirements and challenges to storage and  computing infrastructures. Researchers are not only doing batch loading and  processing of large-scale data, but also demanding the capabilities of  incremental updating and interactive analysis. Therefore, extending existing  storage systems to handle these new requirements becomes an important research  challenge.<br />
  In this exercise, we will introduce  technologies related to data-intensive computing and storage, namely Hadoop and  HBase.</p>

<h1>Video</h1>
<div>
    <iframe width="560" height="315" src="//www.youtube.com/embed/WIZZvVReU3s" frameborder="0" allowfullscreen></iframe>
</div>

<h2>Hadoop  MapReduce</h2>
<p>The Apache Hadoop MapReduce software library  is a framework that allows for the distributed processing of large datasets  across clusters of computers using simple programming models. It is designed to  scale up from single servers to thousands of machines, each offering local  computation and storage. Rather than rely on hardware to deliver  high availability, the library itself is designed to detect and handle failures  at the application layer, delivering a highly available service on top of a  cluster of computers, each of which may be prone to failures [2].<br />
  Hadoop has an architecture consisting of a  master node with many client workers and uses a global queue for task  scheduling, thus achieving natural load balancing among the tasks. The MapReduce  model reduces the data transfer overheads by overlapping data communication  with computations when reduce steps are involved. Hadoop performs duplicate  executions of slower tasks and handles failures by rerunning the failed tasks  using different workers. Data is stored on the Hadoop Distributed File System (HDFS),  a distributed parallel file system for data storage, which stores the data  across the local disks of the computing nodes while presenting a single file  system view through the HDFS API. The architecture is shown in Figure 1.<br />
  <p align="center"><img src="assets/img/hw/se1/1.png" alt="" width="211" height="206" /><br />
  <em>Figure 1</em>. Hadoop MapReduce Architecture<br />
  </p>
 <p>This practice session  and upcoming projects mainly use Hadoop MapReduce as our programming model and  execution framework.</p>

<h2>HBase</h2>
<p>HBase is an open source, distributed,  column-oriented, and sorted-map datastore modeled after Google&rsquo;s BigTable.  Figure 2 shows the data model of HBase. Data is stored in tables; each table  contains multiple rows and a fixed number of column families. For every row,  there can be a varied amount of qualifiers within a column family, and at the intersections of rows and qualifiers are table cells. Cell contents are both uninterpreted and versioned arrays of bytes. A table can be configured to  maintain a certain number of versions for its cell contents. Rows are sorted by  row keys, which are implemented as byte arrays [3-4]. <br />
  <p align="center"><img src="assets/img/hw/se1/2.png" alt="" width="576" height="221" /><br />
  <em>Figure 2</em>. An example of HBase table structure<br />
  </p>
  This practice and  upcoming projects use HBase as a datastore which stores the raw input data and  the output results, even serving as a database for our search engine. </p>
<h1>Exercise:  Create Tables and Load Data to HBase </h1>
<p>This section  provides command-line steps to configure the working environment, start Hadoop  and HBase, create HBase tables, load data into HBase, and view the uploaded  records. Note that these steps must be executed under the prepared <a href="http://cloudmooc.soic.indiana.edu:10002/mooc-ubuntu1204.ova">VirtualBox Image</a> [5].  </p>
<h2>Start Hadoop and HBase </h2>
<p>The following steps  direct you to the locations of Hadoop and HBase and give instructions on how to start them. For Hadoop, a  provided one-click shell script, salsaHadoop, is used to automatically start  Hadoop framework. For HBase, we use the default starter. </p>
<div>
  <pre style="overflow: auto; width: auto;">
  # start hadoop
  $ cd /root/software/hadoop-1.1.2/
  $ . ./MultiNodesOneClickStartUp.sh /root/software/jdk1.6.0_33/ nodes

  # start hbase
  $ cd /root/software/hbase-0.94.7/
  $ ./bin/start-hbase.sh
                </pre>
  </div>
<h2>Configure  the working environment</h2>
<p>Once Hadoop and  HBase have started, we need to copy a configuration file hbase-site.xml to  Hadoop&rsquo;s conf directory and update the environment parameter HADOOP_CLASSPATH. </p>
<div>
            <pre style="overflow: auto; width: auto;">
# prepare for hadoop and hbase environment

$ cp /root/software/hbase-0.94.7/conf/hbase-site.xml /root/software/hadoop-1.1.2/conf/
$ cd /root/software/hadoop-1.1.2/
$ export HADOOP_CLASSPATH=`/root/software/hbase-0.94.7/bin/hbase classpath`
            </pre>
  </div>
<h2>Load data into HBase Tables </h2>
<p>After the working environment is ready, two  HBase Tables, WordCountTable and clueWeb09DataTable for this homework, are created  by the provided java class iu.pti.hbaseapp.clueweb09.TableCreatorClueWeb09. Then we upload the clueWeb09 data to HBase  using helper iu.pti.hbaseapp.clueweb09.DataLoaderClueWeb09.</p>
<div>
            <pre style="overflow: auto; width: auto;">

export HADOOP_CLASSPATH=`/root/software/hbase-0.94.7/bin/hbase classpath`
            
# create hbase tables
$ ./bin/hadoop jar lib/cglHBaseMooc.jar iu.pti.hbaseapp.clueweb09.TableCreatorClueWeb09

# create one directory for mapreduce data input
$ mkdir -p /root/MoocHomeworks/HBaseWordCount/data/clueweb09/mrInput

# create input’s metadata for HBbase data loader
$ ./bin/hadoop jar lib/cglHBaseMooc.jar iu.pti.hbaseapp.clueweb09.Helpers create-mr-input /root/MoocHomeworks/HBaseWordCount/data/clueweb09/files/ /root/MoocHomeworks/HBaseWordCount/data/clueweb09/mrInput/ 1

# copy metadata to Hadoop HDFS
$ ./bin/hadoop dfs -copyFromLocal /root/MoocHomeworks/HBaseWordCount/data/clueweb09/mrInput/ /cw09LoadInput
$ ./bin/hadoop dfs -ls /cw09LoadInput

# load data into HBase (takes 10-20 minutes to finish)
$ ./bin/hadoop jar lib/cglHBaseMooc.jar iu.pti.hbaseapp.clueweb09.DataLoaderClueWeb09 /cw09LoadInput
            </pre>
</div>
<h2>Verify data record from HBase output </h2>
<p>Finally, we verify  the uploaded records with the iu.pti.hbaseapp.clueweb09.DataLoaderClueWeb09 from the clueWeb09DataTable. The screen redirects the output to dataTable1.txt. This file is used for evaluation and must  be uploaded to the MOOC submission page.</p>
<div><pre>
  $ ./bin/hadoop jar lib/cglHBaseMooc.jar iu.pti.hbaseapp.HBaseTableReader  clueWeb09DataTable details string string string string 1 &gt; dataTable1.txt<br />
  $ vim dataTable1.txt </pre>
  </div>
<h1>What&rsquo;s next?</h1>
<p>You are ready to  program your own HBase WordCount. </p>
<h1>References</h1>
<ol>
  <li>Clueweb09  project, <a href="http://lemurproject.org/clueweb09/">http://lemurproject.org/clueweb09/</a></li>
  <li>Hadoop  official website, <a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></li>
  <li>HBase  official website, <a href="http://hbase.apache.org/">http://hbase.apache.org/</a></li>
  <li>Xiaoming Gao, Vaibhav Nachankar, and Judy Qiu.  2011. Experimenting lucene index on HBase in an HPC environment. In <em>Proceedings of the first annual workshop on  High performance computing meets databases</em> (HPCDB '11). ACM, New York, NY,  USA, 25-28. DOI=10.1145/2125636.2125646 <a href="http://doi.acm.org/10.1145/2125636.2125646">http://doi.acm.org/10.1145/2125636.2125646</a></li>
  <li>Cloud  MOOC VirtualBox, http://cloudmooc.soic.indiana.edu:10002/mooc-ubuntu1204.ova</li>
</ol>


